{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7H--nIXQREFP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2294,
     "status": "ok",
     "timestamp": 1761206391165,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "7H--nIXQREFP",
    "outputId": "2607edf4-8f52-4ea3-863e-e1e067d051d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_PATH = \".DATA\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "\n",
    "  DRIVE_PATH = os.path.join(\"content\", \"drive\")\n",
    "\n",
    "  drive.mount(DRIVE_PATH)\n",
    "\n",
    "  BASE_PATH = os.path.join(DRIVE_PATH, \"MyDrive\", \"TP_MACHINE_LEARNING\")\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8592281d",
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1761206391174,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "8592281d"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fcbae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761206391177,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "e3fcbae9",
    "outputId": "6a3bc627-ce19-471c-ffd5-4e37e9ff2ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa82ace",
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1761206391262,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "faa82ace"
   },
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(os.path.join(BASE_PATH, \"DATASET\", \"train\"), transform=train_tf)\n",
    "val_ds = datasets.ImageFolder(os.path.join(BASE_PATH, \"DATASET\", \"val\"), transform=val_tf)\n",
    "test_ds = datasets.ImageFolder(os.path.join(BASE_PATH, \"DATASET\", \"test\"), transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "test_loader = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "UogszPdIh-_2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761206391272,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "UogszPdIh-_2"
   },
   "outputs": [],
   "source": [
    "RUNS_PATH = os.path.join(BASE_PATH, \"RUNS\", \"RESNET\")\n",
    "\n",
    "os.makedirs(RUNS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7badb4f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3210,
     "status": "ok",
     "timestamp": 1761206394489,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "7badb4f1",
    "outputId": "83a35778-9050-4d86-f8d1-5bb43d160c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(RUNS_PATH, \"CHECKPOINT.pt\")\n",
    "\n",
    "CHECKPOINT = dict(E=100, Ei=0, Estag=0, training_classification_header=True, training_backbone=False, best_metric=-torch.inf)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    CHECKPOINT = torch.load(CHECKPOINT_PATH)\n",
    "\n",
    "if \"W\" in CHECKPOINT:\n",
    "    model.load_state_dict(CHECKPOINT[\"W\"], strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = CHECKPOINT[\"training_backbone\"]\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = CHECKPOINT[\"training_classification_header\"]\n",
    "\n",
    "params_to_optimize = list()\n",
    "\n",
    "if CHECKPOINT[\"training_backbone\"]:\n",
    "    params_to_optimize += [ p for n, p in model.named_parameters() if n not in [ \"fc.weight\", \"fc.bias\" ] ]\n",
    "\n",
    "if CHECKPOINT[\"training_classification_header\"]:\n",
    "    params_to_optimize += list(model.fc.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3)\n",
    "\n",
    "if \"optimizer\" in CHECKPOINT:\n",
    "    optimizer.load_state_dict(CHECKPOINT[\"optimizer\"])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc0c24",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761206394504,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "43dc0c24"
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(train_ds.targets)\n",
    "\n",
    "class_counts = torch.bincount(labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258bb376",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761206394557,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "258bb376"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim.Optimizer, criterion: nn.Module, loader: DataLoader):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss: torch.Tensor = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss = sum_loss + loss.item()\n",
    "\n",
    "    return sum_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c281c204",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761206394559,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "c281c204"
   },
   "outputs": [],
   "source": [
    "def eval(model: nn.Module, loader: torch.utils.data.DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = list(), list()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            total_loss += criterion(outputs, labels).item() * labels.size(0)\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    loss = total_loss / len(all_labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(all_labels, all_preds),\n",
    "        \"precision_macro\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0),\n",
    "        \"loss\": loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517b87f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 17883,
     "status": "error",
     "timestamp": 1761206412442,
     "user": {
      "displayName": "Henrique Hott",
      "userId": "10821123899463662620"
     },
     "user_tz": 180
    },
    "id": "9517b87f",
    "outputId": "edaf509e-cf02-42af-847a-b95926950694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "train_log_path = os.path.join(RUNS_PATH, \"train_log.csv\")\n",
    "val_log_path = os.path.join(RUNS_PATH, \"val_log.csv\")\n",
    "\n",
    "if not os.path.exists(train_log_path):\n",
    "    with open(train_log_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([ \"epoch\", \"loss\" ])\n",
    "\n",
    "if not os.path.exists(val_log_path):\n",
    "    with open(val_log_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\n",
    "            \"epoch\",\n",
    "            \"loss\",\n",
    "            \"accuracy\",\n",
    "            \"balanced_accuracy\",\n",
    "            \"precision_macro\",\n",
    "            \"recall_macro\",\n",
    "            \"f1_macro\",\n",
    "            \"f1_weighted\",\n",
    "            \"test_loss\",\n",
    "            \"test_accuracy\",\n",
    "            \"test_balanced_accuracy\",\n",
    "            \"test_precision_macro\",\n",
    "            \"test_recall_macro\",\n",
    "            \"test_f1_macro\",\n",
    "            \"test_f1_weighted\"\n",
    "        ])\n",
    "\n",
    "for epoch in tqdm.tqdm(range(CHECKPOINT[\"Ei\"], CHECKPOINT[\"E\"]), leave=False):\n",
    "    if CHECKPOINT[\"Estag\"] >= 10:\n",
    "        CHECKPOINT[\"Eearly\"] = epoch\n",
    "        break\n",
    "\n",
    "    train_loss = train(model=model, optimizer=optimizer, loader=train_loader, criterion=criterion)\n",
    "    metrics = eval(model=model, loader=val_loader, criterion=criterion)\n",
    "    test_metrics = eval(model=model, loader=test_loader, criterion=criterion)\n",
    "\n",
    "    with open(train_log_path, \"a\", newline=\"\") as f_train, open(val_log_path, \"a\", newline=\"\") as f_val:\n",
    "        csv.writer(f_train).writerow([epoch + 1, train_loss])\n",
    "        csv.writer(f_val).writerow([\n",
    "            epoch + 1,\n",
    "            metrics[\"loss\"],\n",
    "            metrics[\"accuracy\"],\n",
    "            metrics[\"balanced_accuracy\"],\n",
    "            metrics[\"precision_macro\"],\n",
    "            metrics[\"recall_macro\"],\n",
    "            metrics[\"f1_macro\"],\n",
    "            metrics[\"f1_weighted\"],\n",
    "            test_metrics[\"loss\"],\n",
    "            test_metrics[\"accuracy\"],\n",
    "            test_metrics[\"balanced_accuracy\"],\n",
    "            test_metrics[\"precision_macro\"],\n",
    "            test_metrics[\"recall_macro\"],\n",
    "            test_metrics[\"f1_macro\"],\n",
    "            test_metrics[\"f1_weighted\"]\n",
    "        ])\n",
    "        f_train.flush(); f_val.flush()\n",
    "        os.fsync(f_train.fileno()); os.fsync(f_val.fileno())\n",
    "\n",
    "\n",
    "    if metrics[\"balanced_accuracy\"] > CHECKPOINT[\"best_metric\"]:\n",
    "        CHECKPOINT[\"best_metric\"] = metrics[\"balanced_accuracy\"]\n",
    "        CHECKPOINT[\"best_weights\"] = model.state_dict()\n",
    "\n",
    "        tqdm.tqdm.write(f\"Best model updated at epoch {epoch + 1} -> balanced_acc = {CHECKPOINT['best_metric']:.4f}\")\n",
    "\n",
    "        CHECKPOINT[\"Estag\"] = 0\n",
    "    else:\n",
    "        CHECKPOINT[\"Estag\"] = CHECKPOINT[\"Estag\"] + 1\n",
    "\n",
    "    CHECKPOINT[\"Ei\"] = epoch + 1\n",
    "    CHECKPOINT[\"W\"] = model.state_dict()\n",
    "    CHECKPOINT[\"optimizer\"] = optimizer.state_dict()\n",
    "\n",
    "    torch.save(CHECKPOINT, CHECKPOINT_PATH)\n",
    "\n",
    "CHECKPOINT.pop(\"optimizer\")\n",
    "CHECKPOINT.pop(\"Estag\")\n",
    "\n",
    "torch.save(CHECKPOINT, CHECKPOINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tpml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
